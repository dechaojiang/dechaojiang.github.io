<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Libot | Dechao (Cheney) Jiang </title> <meta name="author" content="Dechao (Cheney) Jiang"> <meta name="description" content="Autonomous Mobile Manipulation Robot for Libraries"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A4%AF&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://dechaojiang.github.io/projects/Libot/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/swiper@11.0.5/swiper-bundle.min.css" integrity="sha256-yUoNxsvX+Vo8Trj3lZ/Y5ZBf8HlBFsB6Xwm7rH75/9E=" crossorigin="anonymous"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Dechao</span> (Cheney) Jiang </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/photography/">photography </a> </li> <li class="nav-item "> <a class="nav-link" href="/resume/">Résumé </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Libot</h1> <p class="post-description">Autonomous Mobile Manipulation Robot for Libraries</p> </header> <article> <p>I would like to express my gratitude to Professor Qingsong Xu for his invaluable guidance. Special thanks to my partner, <a href="https://www.linkedin.com/in/tong-zhang-399891227/" rel="external nofollow noopener" target="_blank">Mr. Zhang Tong</a>, for his dedication and countless hours spent discussing and debugging code with me in the FST AI room.</p> <p>Libot is a mobile robot designed to automate tasks in libraries.</p> <p>Visit my gitrepo for codewise ideas: <a href="https://github.com/dechaojiang/24EME_FYP" rel="external nofollow noopener" target="_blank">24EME_FYP</a></p> <h2 id="key-features">Key Features:</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/Libot480.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""></video> </figure> </div> </div> <div class="caption"> Libot in Gazebo simulation </div> <ul> <li> <strong>Book Detection System:</strong> Utilizes the YOLO v8 deep learning model to recognize book spine labels from RGB images, facilitating efficient book cataloging.</li> <li> <strong>Deep-learning-based Object grasping:</strong> Integrated with the <a href="https://moveit.ros.org/" rel="external nofollow noopener" target="_blank">MoveIt</a> platform through move group API to optimize book handling through precise pick-and-place operations. Deep-learning algorithm <a href="https://github.com/atenpas/gpd?tab=readme-ov-file" rel="external nofollow noopener" target="_blank">GPD</a> by <a href="https://www.khoury.northeastern.edu/home/atp/" rel="external nofollow noopener" target="_blank">Andreas</a> is used for grasp generation.</li> <li> <strong>SLAM Mapping and Navigation:</strong> Enhanced with Simultaneous Localization and Mapping (SLAM) to adapt to dynamic library layouts, improving navigation and operational efficiency. Mapping through hectoring mapping by <a href="https://www.teamhector.de/" rel="external nofollow noopener" target="_blank">TeamHector</a> at TU Darmstadt.</li> </ul> <hr> <h2 id="hardware">Hardware</h2> <p>Libot’s hardware consists of three main modules: vision sensor, manipulator, and navigator. These modules are connected to a central control computer.</p> <div class="row justify-content-sm-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Libot/Libot_hw_diag-480.webp 480w,/assets/img/Libot/Libot_hw_diag-800.webp 800w,/assets/img/Libot/Libot_hw_diag-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/Libot/Libot_hw_diag.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-4 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Libot/Libot_outline_demo-480.webp 480w,/assets/img/Libot/Libot_outline_demo-800.webp 800w,/assets/img/Libot/Libot_outline_demo-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/Libot/Libot_outline_demo.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> You can also have artistically styled 2/3 + 1/3 images, like these. </div> <ul> <li>Navigator Module: Utilizes the <strong>MiR250</strong> mobile robot, providing mobility and a customized book sink. The book sink serves as both a storage unit for books and a mounting platform for the manipulator.</li> <li>Manipulator Module: Features a <strong>UR5</strong> robotic arm mounted on the book sink. Attached to the UR5 are: <ul> <li> <strong>Robotiq 2F-140</strong> Gripper: Enables Libot to grasp and manipulate objects.</li> <li> <strong>Intel RealSense D435i</strong> Depth Camera: Part of the vision sensor module, mounted on the UR5’s third wrist through a dedicated holder, providing real-time depth perception.</li> </ul> </li> </ul> <hr> <h2 id="control-methodology--simulation-environment">Control Methodology &amp; Simulation Environment</h2> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Libot/fsm-480.webp 480w,/assets/img/Libot/fsm-800.webp 800w,/assets/img/Libot/fsm-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/Libot/fsm.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Libot's working logic </div> <p>Above shows the conceptual program design of Libot. There are four states</p> <ul id="fsm" class="tab" data-tab="8233423a-2899-456c-bfe9-32a5ebd0be06" data-name="fsm"> <li class="active" id="fsm-idle"> <a href="#">IDLE </a> </li> <li id="fsm-book-indication"> <a href="#">Book_Indication </a> </li> <li id="fsm-p2p-navigation"> <a href="#">P2P_Navigation </a> </li> <li id="fsm-pick-place"> <a href="#">Pick_Place </a> </li> </ul> <ul class="tab-content" id="8233423a-2899-456c-bfe9-32a5ebd0be06" data-name="fsm"> <li class="active"> <ul> <li> <code class="language-plaintext highlighter-rouge">IDLE</code> means Libot not being used or active. No task is execute during Libot’s IDLE state</li> </ul> </li> <li> <ul> <li>Detection of book placed in booksink can trigger the transition from <code class="language-plaintext highlighter-rouge">IDLE</code> state to <code class="language-plaintext highlighter-rouge">Book_Indication</code> state. This transition will output tag information recognized by our vision model 3.16 of the newly placed book and stored in a shared memory. Ultimately, the tag info will be compared with a library database then return the location information of the book, <code class="language-plaintext highlighter-rouge">Indication task</code> completes by this point.</li> </ul> </li> <li> <ul> <li>This state execute the task of the navigation to a given point. It has three entries corresponding to three other states. From <code class="language-plaintext highlighter-rouge">Book_Indication</code> to this state requires the condition of the booksink is full, the first location point which obtained from indication task store in a queue (shared memory) will be assign to a variable name <code class="language-plaintext highlighter-rouge">target_point</code> which is the point will be given to navigation task. Direct transition from <code class="language-plaintext highlighter-rouge">IDLE</code> to this state is possible, this design is to make Libot work at a certain frequency. For example, the <code class="language-plaintext highlighter-rouge">timeout</code> value can set to be 4 hours, with this entry to navigation, Libot will be in return book operation every 4 hours even the booksink is not full. Onceit enters <code class="language-plaintext highlighter-rouge">P2P_Navigation</code> from <code class="language-plaintext highlighter-rouge">IDLE</code>, a re-entry will be trigger with the condition of the booksink is not full, it will assign <code class="language-plaintext highlighter-rouge">target_point</code> in the way mentioned earlier.</li> </ul> </li> <li> <ul> <li>After the complement of the navigation task, Libot will be in this state to perform <code class="language-plaintext highlighter-rouge">Pick and Place task</code>, upon the completion of this task, it will returns to <code class="language-plaintext highlighter-rouge">P2P_Navigation</code> state. Please scroll down to see detailed description of Pick and Place task.</li> </ul> </li> </ul> <p>Once the Libot return all the books after the transition between <code class="language-plaintext highlighter-rouge">P2P_Navigation</code> and <code class="language-plaintext highlighter-rouge">Pick_Place</code> state, the emptiness of booksink will trigger the re-entry of assigning the <code class="language-plaintext highlighter-rouge">target_point</code> to HOME position. Libot will move to it’s HOME position first and turn to <code class="language-plaintext highlighter-rouge">IDLE</code> again.</p> <hr> <h3 id="collaborating-gazebo-and-ros-control">Collaborating Gazebo and ROS control</h3> <p>Libot was developed and tested in the simulation platform <a href="https://gazebosim.org/home" rel="external nofollow noopener" target="_blank">Gazebo</a>. Physical parameters like mass, inertia, were defined and fine-tuned through our Universal Robotic Description Format (URDF) files for Libot, and pass on to Gazebo to mimic the real-world situation. Gazebo can read and write the <code class="language-plaintext highlighter-rouge">hardware_interface::RobotHWSim</code> provided by <code class="language-plaintext highlighter-rouge">ros_control</code> package which enables the reflection of control by ROS in Gazebo simulation.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Libot/Data_flow_of_ros_control_Gazebo_reality-480.webp 480w,/assets/img/Libot/Data_flow_of_ros_control_Gazebo_reality-800.webp 800w,/assets/img/Libot/Data_flow_of_ros_control_Gazebo_reality-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/Libot/Data_flow_of_ros_control_Gazebo_reality.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Dataflow of ros_control, Gazebo and reality in Libot test </div> <hr> <h3 id="mapping--point2point-navigation">Mapping &amp; Point2Point Navigation</h3> <p>MiR is a commercial product with its own mapping and navigation technology. However, Libot uses the open-source system ROS and its packages for navigation. The key component is the move_base node from the <code class="language-plaintext highlighter-rouge">ros_navigation</code> package, enhanced by <code class="language-plaintext highlighter-rouge">hector mapping</code> for SLAM. This approach utilizes a LiDAR and depth cameras on Libot’s MiR base.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Libot/nav_overview-480.webp 480w,/assets/img/Libot/nav_overview-800.webp 800w,/assets/img/Libot/nav_overview-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/Libot/nav_overview.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> ROS structure of Libot's navigation </div> <ul id="nav" class="tab" data-tab="c2cbcb75-51c4-4771-b7d3-f04bfd1dde19" data-name="nav"> <li class="active" id="nav-input"> <a href="#">input </a> </li> <li id="nav-tf"> <a href="#">TF </a> </li> <li id="nav-costmap"> <a href="#">costmap </a> </li> <li id="nav-planner"> <a href="#">planner </a> </li> <li id="nav-execution"> <a href="#">Execution </a> </li> <li id="nav-mapping"> <a href="#">Mapping </a> </li> <li id="nav-recovery"> <a href="#">Recovery </a> </li> <li id="nav-hi"> <a href="#">HI </a> </li> </ul> <ul class="tab-content" id="c2cbcb75-51c4-4771-b7d3-f04bfd1dde19" data-name="nav"> <li class="active"> <p><code class="language-plaintext highlighter-rouge">Sensory input:</code></p> <ul> <li> <p>Odometry Source: Provides data about Libot’s movement over time, such as distance and speed, based on wheel encoders or other motion sensors.</p> </li> <li> <p>Sensor Sources: Inputs from the robot’s perception hardware, like LIDAR or depth cameras, providing <code class="language-plaintext highlighter-rouge">sensor_msgs/LaserScan</code> and <code class="language-plaintext highlighter-rouge">sensor_msgs/PointCloud</code> data.</p> </li> </ul> </li> <li> <p><code class="language-plaintext highlighter-rouge">Sensor Transforms</code></p> <ul> <li>The data from various sensors are transformed using <code class="language-plaintext highlighter-rouge">tf/tfMessage</code>, which maintains the relationship between coordinate frames. This step aligns all sensory data to a common reference frame, helping Libot understand its environment relative to its position and orientation.</li> </ul> </li> <li> <p><code class="language-plaintext highlighter-rouge">Costmap generation</code></p> <ul> <li>Both the <code class="language-plaintext highlighter-rouge">global_costmap</code> and <code class="language-plaintext highlighter-rouge">local_costmap</code> are updated with the transformed sensor data. The <code class="language-plaintext highlighter-rouge">global_costmap</code> reflects the overall environment based on a pre-existing map, while the <code class="language-plaintext highlighter-rouge">local_costmap</code> is dynamic, reflecting immediate obstacles and changes around Libot.</li> </ul> </li> <li> <p><code class="language-plaintext highlighter-rouge">Global planner</code></p> <ul> <li>Using the <code class="language-plaintext highlighter-rouge">global_costmap</code>, the global_planner computes an initial path to the destination. This path is laid out as a series of waypoints and passed down as <code class="language-plaintext highlighter-rouge">nav_msgs/Path</code> to the <code class="language-plaintext highlighter-rouge">local_planner</code>.</li> </ul> <p><code class="language-plaintext highlighter-rouge">Local planner</code></p> <ul> <li>The <code class="language-plaintext highlighter-rouge">local_planner</code> refines the path provided by the <code class="language-plaintext highlighter-rouge">global_planner</code>, considering real-time data from the <code class="language-plaintext highlighter-rouge">local_costmap</code>. It generates a short-term path for Libot to follow safely, adjusting for unexpected obstacles and ensuring the robot’s movements are kinematically feasible.</li> </ul> </li> <li> <p><code class="language-plaintext highlighter-rouge">Execution of movement</code></p> <ul> <li>Movement commands, encapsulated in <code class="language-plaintext highlighter-rouge">cmd_vel</code> messages of type <code class="language-plaintext highlighter-rouge">geometry_msgs/Twist</code>, are sent from the <code class="language-plaintext highlighter-rouge">local_planner</code> to the MiR controller. This component controls Libot’s actuators, translating the velocity commands into physical motion.</li> </ul> </li> <li> <p><code class="language-plaintext highlighter-rouge">Hector Mapping</code></p> <ul> <li>In parallel, the <code class="language-plaintext highlighter-rouge">hector_mapping</code> node performs SLAM to update the map of the environment and locate Libot within it. This information updates the <code class="language-plaintext highlighter-rouge">global_costmap</code> and aids in continuous navigation.</li> </ul> </li> <li> <p><code class="language-plaintext highlighter-rouge">Recovery behaviour</code></p> <ul> <li>If Libot encounters a problem, like an impassable obstacle or a localization error, the <code class="language-plaintext highlighter-rouge">recovery_behaviors</code> are triggered. These behaviors aim to resolve the issue by re-planning a path or clearing the costmaps to restart the mapping process.</li> </ul> </li> <li> <p><code class="language-plaintext highlighter-rouge">Human interaction</code></p> <ul> <li>Throughout this process, a human operator can intervene by providing new commands or interrupting the current operation.</li> </ul> </li> </ul> <hr> <h3 id="computer-vision-book-detection">Computer Vision Book Detection</h3> <swiper-container keyboard="true" navigation="true" pagination="true" pagination-clickable="true" pagination-dynamic-bullets="true" rewind="true"> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Libot/yolo_chart-480.webp 480w,/assets/img/Libot/yolo_chart-800.webp 800w,/assets/img/Libot/yolo_chart-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/Libot/yolo_chart.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Libot/yolo_table" sizes="95vw"></source> <img src="/assets/img/Libot/yolo_table" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </swiper-slide> </swiper-container> <p>Dectection results trained by <code class="language-plaintext highlighter-rouge">YOLOv8l</code> model are illustrated in Table 4.2. The model showcases its capability to effectively detect objects with various degrees of overlapping bounding boxes, as evidenced by an overall class mAP50 of 92.3% and a noteworthy mAP50-95 of 82.8% among all string classes.</p> <p>Many Thanks to my partner <strong>Zhang Tong</strong> who resolve all ML dependencies and trained the model.</p> <hr> <h3 id="grasp-pose-detection-gpd">Grasp Pose Detection (GPD)</h3> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Libot/gpd-480.webp 480w,/assets/img/Libot/gpd-800.webp 800w,/assets/img/Libot/gpd-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/Libot/gpd.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Task diagram of GPD pick&amp;place </div> <p>Within sequence showing above, there are three core subprocesses outlined:</p> <ul id="gpd" class="tab" data-tab="44dbc6ee-c4eb-4525-96b9-9ffa284ea268" data-name="gpd"> <li class="active" id="gpd-gpd"> <a href="#">GPD </a> </li> <li id="gpd-tf"> <a href="#">TF </a> </li> <li id="gpd-p-p"> <a href="#">P&amp;P </a> </li> </ul> <ul class="tab-content" id="44dbc6ee-c4eb-4525-96b9-9ffa284ea268" data-name="gpd"> <li class="active"> <ul> <li> <code class="language-plaintext highlighter-rouge">GPD Algorithm:</code> This module processes the <code class="language-plaintext highlighter-rouge">PointCloud2</code> data, received as a ROS topic, to compute potential grasps. These grasps are identified and then published as a <code class="language-plaintext highlighter-rouge">clustered_grasps</code> topic, which aggregates the grasping points identified by the GPD algorithm.</li> </ul> </li> <li> <ul> <li> <code class="language-plaintext highlighter-rouge">TF Pose Transform:</code> Subsequent to grasp identification, the <code class="language-plaintext highlighter-rouge">clustered_grasps</code> must undergo a transformation to align with the arm of Libot’s operational frame of reference. This step involves converting the grasps from the <code class="language-plaintext highlighter-rouge">camera_depth_optical_frame</code> to the <code class="language-plaintext highlighter-rouge">booksink_link</code>, which serves as the reference frame for the MoveIt motion planning framework. This transformation is pivotal as it ensures the grasps are contextualized within the robot’s spatial understanding and MoveIt’s planning pipeline.</li> </ul> </li> <li> <ul> <li> <code class="language-plaintext highlighter-rouge">Simple Pick Place Task:</code> The final module, depicted as a rounded rectangle, utilizes the transformed grasp data to inform the motion planning for the UR5 robotic arm. This <code class="language-plaintext highlighter-rouge">simple_pick_place.py</code> script orchestrates the physical actions required for the robotic arm to move to the desired positions and execute the pick and place task, finally returns a new state to <code class="language-plaintext highlighter-rouge">Libot State</code> in a rectangle which represents the shared memory that can be pass to and edit in other state.</li> </ul> </li> </ul> <hr> <h3 id="manipulation-planning-with-moveit">Manipulation planning with MoveIt!</h3> <p>Learn more about MoveIt: <a href="https://moveit.ros.org/" rel="external nofollow noopener" target="_blank">MoveIt motion planning network</a></p> <p><strong>Work done by this project through MoveIt:</strong></p> <ul> <li>Two move groups set up using moveit setup assistant, generated a ROS package powering Libot’s manipulation task MoveIt planning pipeline.</li> <li>Integration with Deep Learning based Grasp Pose Detection</li> </ul> <p><strong>Libot’s control through MoveIt can be done in several ways:</strong></p> <ul> <li>MoveIt Commander: This includes the MoveIt Python API and the MoveIt Command Line Tool for programmatic control of Libot’s movements.</li> <li>ROS Visualizer (RViz): An interactive tool that lets users visually plan and execute trajectories within a simulated environment.</li> </ul> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Dechao (Cheney) Jiang. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script defer src="https://cdn.jsdelivr.net/npm/swiper@11.0.5/swiper-element-bundle.min.js" integrity="sha256-BPrwikijIybg9OQC5SYFFqhBjERYOn97tCureFgYH1E=" crossorigin="anonymous"></script> <script src="/assets/js/tabs.min.js?b8748955e1076bbe0dabcf28f2549fdc"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>